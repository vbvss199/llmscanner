<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="LLM Vulnerability Scanner helps secure AI models by identifying and mitigating potential risks in large language models.">
    <meta name="keywords" content="LLM, AI security, vulnerability scanner, language models">
    <meta property="og:title" content="LLM Vulnerability Scanner">
    <meta property="og:description" content="Secure your AI models with our comprehensive vulnerability scanning tool.">
    <meta property="og:image" content="/dashboard-preview.png">
    <meta property="og:url" content="https://your-site.com">
    <title>LLM Vulnerability Scanner</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;700&display=swap" rel="stylesheet">
    <link href="https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css" rel="stylesheet">
    <link href="https://unpkg.com/aos@2.3.1/dist/aos.css" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css" rel="stylesheet" integrity="sha512-DTOQO9RWCH3ppGqcWaEA1BIZOC6xxalwEsw9c2QQeAIftl+Vegovlnee1c9QX4T Fixture for FAQ icons">
    <link href="style.css" rel="stylesheet">
</head>
<body class="font-inter text-gray-800 bg-gray-50">
    <div id="app">
        <!-- Navigation -->
        <nav id="navbar" class="fixed top-0 left-0 right-0 z-50 transition-colors duration-300">
            <div class="container mx-auto px-4">
                <div class="flex items-center justify-between h-16">
                    <a href="#" class="flex items-center space-x-2" aria-label="LLM Scanner Home">
                        <img src="shield-check.svg" alt="Logo" class="w-8 h-8">
                        <span class="text-white font-bold text-xl">LLM Scanner</span>
                    </a>
                    <div class="hidden md:flex space-x-8">
                        <a href="#features" class="text-white hover:text-blue-200 transition-colors" aria-label="View Features Section">Features</a>
                        <a href="#documentation" class="text-white hover:text-blue-200 transition-colors" aria-label="View Documentation Section">Documentation</a>
                        <a href="#deployment" class="text-white hover:text-blue-200 transition-colors" aria-label="View Deployment Section">Deployment</a>
                        <a href="#team" class="text-white hover:text-blue-200 transition-colors" aria-label="View Team Section">Team</a>
                    </div>
                    <button id="mobile-menu-button" class="md:hidden text-white" aria-expanded="false" aria-controls="mobile-menu">
                        <svg class="w-6 h-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                            <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M4 6h16M4 12h16m-16 6h16"></path>
                        </svg>
                    </button>
                </div>
            </div>
            <div id="mobile-menu" class="hidden md:hidden bg-blue-900">
                <div class="px-2 pt-2 pb-3 space-y-1">
                    <a href="#features" class="block px-3 py-2 text-white hover:bg-blue-800 rounded-md">Features</ Rocks
                    <a href="#documentation" class="block px-3 py-2 text-white hover:bg-blue-800 rounded-md">Documentation</a>
                    <a href="#deployment" class="block px-3 py-2 text-white hover:bg-blue-800 rounded-md">Deployment</a>
                    <a href="#team" class="block px-3 py-2 text-white hover:bg-blue-800 rounded-md">Team</a>
                </div>
            </div>
        </nav>

       <!-- Hero Section -->
<section id="hero" class="relative min-h-screen bg-gradient-to-br from-blue-900 to-blue-800 text-white pt-16">
    <div class="container mx-auto px-4 z-10">
        <div class="flex flex-col md:flex-row items-center">
            <div class="md:w-1/2 text-center md:text-left" data-aos="fade-right">
                <h1 class="text-4xl md:text-6xl font-bold mb-6">
                    Secure Your <span class="text-orange-500">AI Models</span> Against Vulnerabilities
                </h1>
                <p class="text-xl mb-8">
                    Our LLM Vulnerability Scanner provides comprehensive security testing for large language models, helping you identify and mitigate potential risks before they become threats.
                </p>
                <div class="flex flex-col sm:flex-row gap-4 justify-center md:justify-start">
                    <a href="#documentation" class="btn-primary">Documentation</a>
                    <a href="https://funokfwehd5wiegljwxtnq.streamlit.app/" class="btn-secondary" target="_blank" rel="noopener noreferrer">LLM vul scanner</a>
                </div>
                <!-- Add video links here -->
                <div class="mt-8">
    <h2 class="text-2xl font-bold text-white mb-4">Mile Stones:</h2>
    <ul class="list-none space-y-2 text-white">
        <li><a href="https://youtu.be/sbqVPla-NF8" target="_blank" class="text-orange-400 hover:underline"> milestone 1</a></li>
        <li><a href="https://www.youtube.com/watch?v=Bc_rYRQbYI0" target="_blank" class="text-orange-400 hover:underline">milestone 2</a></li>
    </ul>
</div>

<!-- Final Demo Link -->
<div class="mt-8">
    <h2 class="text-2xl font-bold text-white mb-4">Final Demo:</h2>
    <ul class="list-none space-y-2 text-white">
        <li><a href="https://youtu.be/MSHKJLswErk" target="_blank" class="text-orange-400 hover:underline">Final Demo</a></li>
    </ul>
</div>
            <!-- <div class="md:w-1/2 mt-8 md:mt-0" data-aos="fade-left">
                <img src="/dashboard-preview.png" alt="LLM Scanner Dashboard" class="rounded-lg shadow-xl">
            </div> -->
        </div>
    </div>
</section>

        <!-- Features Section -->
        <section id="features" class="py-20">
            <div class="container mx-auto px-4">
                <h2 class="text-3xl md:text-4xl font-bold text-center mb-12" data-aos="fade-up">Key Features</h2>
                <div class="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-8">
                    <div class="feature-card" data-aos="fade-up">
                        <div class="feature-icon bg-blue-50 text-blue-600">
                            <svg class="w-6 h-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 12l2 2 4-4m5.618-4.016A11.955 11.955 0 0112 2.944a11.955 11.955 0 01-8.618 3.04A12.02 12.02 0 003 9c0 5.591 3.824 10.29 9 11.622 5.176-1.332 9-6.03 9-11.622 0-1.042-.133-2.052-.382-3.016z"></path>
                            </svg>
                        </div>
                        <h3 class="text-xl font-bold mb-2">Advanced Security Testing</h3>
                        <p class="text-gray-600">Comprehensive vulnerability assessment using state-of-the-art testing methodologies.</p>
                    </div>
                    <div class="feature-card" data-aos="fade-up" data-aos-delay="100">
                        <div class="feature-icon bg-teal-50 text-teal-600">
                            <svg class="w-6 h-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M13 10V3L4 14h7v7l9-11h-7z"></path>
                            </svg>
                        </div>
                        <h3 class="text-xl font-bold mb-2">Real-time Analysis</h3>
                        <p class="text-gray-600">Instant feedback and analysis of potential vulnerabilities in your LLM models.</p>
                    </div>
                    <div class="feature-card" data-aos="fade-up" data-aos-delay="200">
                        <div class="feature-icon bg-orange-50 text-orange-600">
                            <svg class="w-6 h-6" fill="none" stroke="currentColor" viewBox="0 0 24 24">
                                <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 19v-6a2 2 0 00-2-2H5a2 2 0 00-2 2v6a2 2 0 002 2h2a2 2 0 002-2zm0 0V9a2 2 0 012-2h2a2 2 0 012 2v10m-6 0a2 2 0 002 2h2a2 2 0 002-2m0 0V5a2 2 0 012-2h2a2 2 0 012 2v14a2 2 0 01-2 2h-2a2 2 0 01-2-2z"></path>
                            </svg>
                        </div>
                        <h3 class="text-xl font-bold mb-2">Detailed Reports</h3>
                        <p class="text-gray-600">Comprehensive reporting with actionable insights and mitigation strategies.</p>
                    </div>
                </div>
            </div>
        </section>

        <!-- Sample Reports Section -->
<section id="sample-reports" class="py-20 bg-gray-50">
    <div class="container mx-auto px-4">
        <h2 class="text-3xl md:text-4xl font-bold text-center mb-12" data-aos="fade-up">Sample Reports Generated by LLM Vulnerability Scanner</h2>
        
        <div class="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-8">
            <!-- Sample Report 1 -->
            <div class="report-card" data-aos="fade-up">
                <div class="bg-white p-6 rounded-lg shadow-md">
                    <h3 class="text-xl font-bold mb-4">Sample Report 1: Denial of Service</h3>
                    <p class="text-gray-600 mb-4">This sample report provides an overview of the security risks identified in the model, including prompt injection vulnerabilities and data leakage risks.</p>
                    <a href="guard_report_Denial of Service_0_f4d3f0a3.txt" target="_blank" class="text-blue-600 hover:underline">View Full Report</a>
                </div>
            </div>
            <!-- Sample Report 2 -->
            <div class="report-card" data-aos="fade-up" data-aos-delay="100">
                <div class="bg-white p-6 rounded-lg shadow-md">
                    <h3 class="text-xl font-bold mb-4">Sample Report 2: Command Execution</h3>
                    <p class="text-gray-600 mb-4">This report highlights critical vulnerabilities, providing detailed insights into risk mitigation strategies, and the model's response to security tests.</p>
                    <a href="guard_report_Command Execution_0_01b2c9be.txt" target="_blank" class="text-blue-600 hover:underline">View Full Report</a>
                </div>
            </div>
            <!-- Sample Report 3 -->
            <div class="report-card" data-aos="fade-up" data-aos-delay="200">
                <div class="bg-white p-6 rounded-lg shadow-md">
                    <h3 class="text-xl font-bold mb-4">Sample Report 3: Prompt</h3>
                    <p class="text-gray-600 mb-4">A comprehensive evaluation of the API vulnerabilities, detailing potential injection points, and how the system mitigates them.</p>
                    <a href="guard_report_Prompt Injection_0_e50bf87c.txt" target="_blank" class="text-blue-600 hover:underline">View Full Report</a>
                </div>
            </div>
        </div>
    </div>
</section>


        <!-- Documentation Section -->
        <section id="documentation" class="py-20 bg-white">
            <div class="container mx-auto px-4">
                <h2 class="text-3xl md:text-4xl font-bold text-center mb-12" data-aos="fade-up">Documentation</h2>
                <div class="grid grid-cols-1 lg:grid-cols-4 gap-8">
                    <div class="lg:col-span-1">
                        <nav class="sticky top-24">
                            <ul class="space-y-2">
                                <li><a href="#getting-started" class="doc-nav-item">Getting Started</a></li>
                                <li><a href="#installation" class="doc-nav-item">Installation</a></li>
                                <li><a href="#configuration" class="doc-nav-item">Configuration</a></li>
                                <li><a href="#usage" class="doc-nav-item">Usage</a></li>
                                <li><a href="#api-reference" class="doc-nav-item">API Reference</a></li>
                            </ul>
                        </nav>
                    </div>
                    <div class="lg:col-span-3">
                        <section id="getting-started" class="documentation-section">
                            <h3 class="text-2xl font-bold mb-4">Getting Started</h3>
                            <p class="mb-4">The LLM Vulnerability Scanner is a powerful tool to identify and mitigate risks in large language models (LLMs). Follow the steps below to set up the scanner and start running vulnerability tests:</p>
                            <ul class="list-disc pl-6">
                                <li>Clone the repository from GitHub:</li>
                                <pre><code class="language-bash">git clone https://github.com/your-repo/llm-scanner</code></pre>
                                <li>Install the required dependencies:</li>
                                <pre><code class="language-bash">pip install -r requirements.txt</code></pre>
                                <li>Set up your environment variables. Create a `.env` file in the root directory and add your OpenAI API key:</li>
                                <pre><code class="language-bash">OPENAI_API_KEY=your_api_key_here</code></pre>
                            </ul>
                            <p>Once you've set up the environment, you're ready to start scanning your models for vulnerabilities!</p>
                        </section>
                        
                        <section id="installation" class="documentation-section">
                            <h3 class="text-2xl font-bold mb-4">Installation</h3>
                            <p class="mb-4">Follow these steps to install and configure the LLM Vulnerability Scanner:</p>
                            <ul class="list-disc pl-6">
                                <li>Clone the repository from GitHub:</li>
                                <pre><code class="language-bash text-gray-800 bg-gray-100 p-4 rounded-md">git clone https://github.com/your-repo/llm-scanner</code></pre>
                                <li>Navigate to the project folder and install the dependencies:</li>
                                <pre><code class="language-bash text-gray-800 bg-gray-100 p-4 rounded-md">cd llm-scanner
                                pip install -r requirements.txt</code></pre>                                
                                <li>Set up your environment variables by creating a `.env` file with the following content:</li>
                                <pre><code class="language-bash text-gray-800 bg-gray-100 p-4 rounded-md">OPENAI_API_KEY=your_api_key_here</code></pre>
                            </ul>
                            <p>Now you're ready to start scanning your models!</p>
                        </section>
                        
                        
                        
                        
                        <section id="configuration" class="documentation-section">
                            <h3 class="text-2xl font-bold mb-4">Configuration</h3>
                            <p class="mb-4">Before running the LLM Vulnerability Scanner, configure it by setting the necessary parameters:</p>
                            <ul class="list-disc pl-6">
                                <li>Specify the target model and provider in the command line or script configuration. Supported providers include OpenAI, Ollama, Gemini, and more.</li>
                                <li>Set the endpoint for the API provider:</li>
                                <pre><code class="language-bash">--endpoint https://api.openai.com/v1/chat/completions</code></pre>
                                <li>Choose which vulnerability categories to scan. Available categories are:</li>
                                <ul class="list-inside list-disc">
                                    <li>Prompt Injection</li>
                                    <li>Jailbreak</li>
                                    <li>Privacy Leak</li>
                                    <li>Denial of Service</li>
                                    <li>Command Execution</li>
                                </ul>
                                <li>Set the Llama Guard endpoint if using the integrated safety tool (optional).</li>
                            </ul>
                            <p>Example configuration for running the scanner with OpenAI:</p>
                            <pre><code class="language-bash">python adaptive_scanner.py --model gpt-3.5 --provider OpenAI --endpoint https://api.openai.com/v1/chat/completions --categories "Prompt Injection" "Jailbreak" --llama-guard</code></pre>
                        </section>
                        
                        <section id="usage" class="documentation-section">
                            <h3 class="text-2xl font-bold mb-4">Usage</h3>
                            <p class="mb-4">Once you have completed the configuration, you can run the vulnerability scanner to test the security of your target LLM. Here's how to use it:</p>
                            <ul class="list-disc pl-6">
                                <li>Run the scanner with the necessary arguments to specify your model, provider, and vulnerability categories:</li>
                                <pre><code class="language-bash">python adaptive_scanner.py --model gpt-3.5 --provider OpenAI --endpoint https://api.openai.com/v1/chat/completions --categories "Prompt Injection" "Jailbreak"</code></pre>
                                <li>The scanner will adapt prompts, check them for vulnerabilities, and evaluate the responses for risks.</li>
                                <li>Reports will be generated, showing the details of vulnerabilities and risk scores.</li>
                                <li>You can also specify whether to use Llama Guard for additional safety checks during the scan.</li>
                            </ul>
                            <p>For detailed results and logs, the scanner will output a report in JSON format that you can review.</p>
                        </section>
                        
                        <!-- API Reference Section -->
<section id="api-reference" class="documentation-section py-20 bg-white">
    <div class="container mx-auto px-4">
        <h2 class="text-3xl md:text-4xl font-bold text-center mb-12" data-aos="fade-up">API Reference</h2>
        
        <!-- API Overview -->
        <div class="mb-12">
            <h3 class="text-xl font-bold mb-4">Overview</h3>
            <p class="text-gray-600">The LLM Vulnerability Scanner interacts with various APIs for different tasks such as generating prompts, evaluating responses, and leveraging Llama Guard for safety evaluations. Below are the key API endpoints used by the scanner.</p>
        </div>

        <!-- API Endpoints -->
        <div class="space-y-12">

            <!-- Endpoint 1: OpenAI Evaluation API -->
            <div>
                <h4 class="text-xl font-bold mb-2">OpenAI Evaluation API</h4>
                <p class="text-gray-600 mb-4">This API is used to send a prompt to OpenAI’s GPT models to evaluate responses for potential security risks.</p>
                <h5 class="text-lg font-semibold mb-2">Endpoint</h5>
                <pre><code class="language-bash">POST https://api.openai.com/v1/chat/completions</code></pre>
                <h5 class="text-lg font-semibold mb-2">Request Body</h5>
                <pre><code class="language-json">
{
  "model": "gpt-3.5-turbo",
  "messages": [
    {
      "role": "user",
      "content": "Evaluate the following LLM response for potential security risk. Prompt: <prompt_here> Response: <response_here>"
    }
  ]
}
                </code></pre>
                <h5 class="text-lg font-semibold mb-2">Response</h5>
                <pre><code class="language-json">
{
  "choices": [
    {
      "message": {
        "content": "<evaluation_here>"
      }
    }
  ]
}
                </code></pre>
                <p class="text-gray-600">This API evaluates the provided prompt and response, returning a security risk evaluation.</p>
            </div>

            <!-- Endpoint 2: Llama Guard Prompt Safety Check -->
            <div>
                <h4 class="text-xl font-bold mb-2">Llama Guard Prompt Safety Check</h4>
                <p class="text-gray-600 mb-4">This endpoint checks whether a generated prompt is safe to send to the target LLM. It leverages Llama Guard for safety evaluations.</p>
                <h5 class="text-lg font-semibold mb-2">Endpoint</h5>
                <pre><code class="language-bash">POST http://localhost:11434/api/chat</code></pre>
                <h5 class="text-lg font-semibold mb-2">Request Body</h5>
                <pre><code class="language-json">
{
  "prompt": "<generated_prompt_here>"
}
                </code></pre>
                <h5 class="text-lg font-semibold mb-2">Response</h5>
                <pre><code class="language-json">
{
  "is_unsafe": false,
  "details": "Prompt is safe"
}
                </code></pre>
                <p class="text-gray-600">This API evaluates whether the generated prompt is considered safe or if it contains potentially harmful content (e.g., prompt injection).</p>
            </div>

            <!-- Endpoint 3: Target LLM API (OpenAI, Gemini, etc.) -->
            <div>
                <h4 class="text-xl font-bold mb-2">Target LLM API</h4>
                <p class="text-gray-600 mb-4">This endpoint sends the generated prompt to the target LLM (e.g., OpenAI, Gemini) and receives the response.</p>
                <h5 class="text-lg font-semibold mb-2">Endpoint</h5>
                <pre><code class="language-bash">POST <target_api_endpoint></code></pre>
                <h5 class="text-lg font-semibold mb-2">Request Body</h5>
                <pre><code class="language-json">
{
  "model": "gpt-3.5",
  "messages": [
    {
      "role": "user",
      "content": "<generated_prompt_here>"
    }
  ]
}
                </code></pre>
                <h5 class="text-lg font-semibold mb-2">Response</h5>
                <pre><code class="language-json">
{
  "choices": [
    {
      "message": {
        "content": "<response_here>"
      }
    }
  ]
}
                </code></pre>
                <p class="text-gray-600">This endpoint interacts with the target LLM (e.g., OpenAI, Gemini) and receives the model’s response based on the provided prompt.</p>
            </div>

            <!-- Endpoint 4: Llama Guard Report Saving -->
            <div>
                <h4 class="text-xl font-bold mb-2">Llama Guard Report Saving</h4>
                <p class="text-gray-600 mb-4">This endpoint is used to save the safety evaluation reports generated by Llama Guard for auditing and record-keeping.</p>
                <h5 class="text-lg font-semibold mb-2">Endpoint</h5>
                <pre><code class="language-bash">POST http://localhost:11434/api/report</code></pre>
                <h5 class="text-lg font-semibold mb-2">Request Body</h5>
                <pre><code class="language-json">
{
  "report": {
    "attack_type": "<attack_type>",
    "prompt": "<prompt_here>",
    "response": "<response_here>",
    "is_unsafe": true,
    "details": "Detected potential prompt injection attack"
  }
}
                </code></pre>
                <h5 class="text-lg font-semibold mb-2">Response</h5>
                <pre><code class="language-json">
{
  "status": "success",
  "message": "Report saved successfully"
}
                </code></pre>
                <p class="text-gray-600">This API endpoint saves the detailed safety report generated by Llama Guard for each prompt and response evaluation.</p>
            </div>

        </div>
    </div>
</section>
                    </div>
                </div>
            </div>
        </section>

       <!-- Deployment Section -->
<section id="deployment" class="py-20 bg-gray-50">
    <div class="container mx-auto px-4">
        <h2 class="text-3xl md:text-4xl font-bold text-center mb-12" data-aos="fade-up">Deployment Guide</h2>
        <div class="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-2 gap-8">
            
            <!-- Local Development Section -->
            <div class="feature-card" data-aos="fade-up">
                <h3 class="text-xl font-bold mb-4">Local Deployment</h3>
                <p class="text-gray-600 mb-4">Set up your development environment and run the scanner locally on your machine.</p>
                <p class="text-gray-600 mb-4">Follow these steps to deploy the scanner locally:</p>
                <ul class="list-disc pl-6 text-gray-600">
                    <li>Clone the repository from GitHub:</li>
                </ul>
                <div class="code-block">
                    <pre><code class="language-bash">
git clone https://github.com/your-repo/llm-scanner
                    </code></pre>
                </div>
                <ul class="list-disc pl-6 text-gray-600">
                    <li>Navigate to the project folder:</li>
                </ul>
                <div class="code-block">
                    <pre><code class="language-bash">
cd llm-scanner
                    </code></pre>
                </div>
                <ul class="list-disc pl-6 text-gray-600">
                    <li>Set up a Python virtual environment and activate it:</li>
                </ul>
                <div class="code-block">
                    <pre><code class="language-bash">
python3 -m venv llm-scanner-env
source llm-scanner-env/bin/activate
                    </code></pre>
                </div>
                <ul class="list-disc pl-6 text-gray-600">
                    <li>Install the required dependencies:</li>
                </ul>
                <div class="code-block">
                    <pre><code class="language-bash">
pip install -r requirements.txt
                    </code></pre>
                </div>
                <ul class="list-disc pl-6 text-gray-600">
                    <li>Set up environment variables by creating a `.env` file in the root directory:</li>
                </ul>
                <div class="code-block">
                    <pre><code class="language-bash">
OPENAI_API_KEY=your-openai-api-key
                    </code></pre>
                </div>
                <p class="text-gray-600 mb-4">Once set up, run the scanner with the following command:</p>
                <div class="code-block">
                    <pre><code class="language-bash">
python adaptive_scanner.py --model gpt-3.5 --provider OpenAI --endpoint https://api.openai.com/v1/chat/completions --categories "Prompt Injection" "Jailbreak"
                    </code></pre>
                </div>
            </div>

            <!-- Cloud Deployment Section (AWS) -->
            <div class="feature-card" data-aos="fade-up" data-aos-delay="100">
                <h3 class="text-xl font-bold mb-4">Cloud Deployment (AWS)</h3>
                <p class="text-gray-600 mb-4">Deploy the scanner on cloud platforms like AWS using EC2. This setup is for those who want to run the scanner in the cloud.</p>
                <p class="text-gray-600 mb-4">Follow the steps to deploy on AWS EC2:</p>
                <ul class="list-disc pl-6 text-gray-600">
                    <li>Launch an EC2 instance (Ubuntu recommended).</li>
                    <li>SSH into your instance: <code>ssh -i /path/to/your-key.pem ubuntu@your-ec2-public-ip</code></li>
                    <li>Install the required dependencies: <code>sudo apt-get install python3 python3-pip git</code></li>
                    <li>Clone the repository and install the required packages:</li>
                </ul>
                <div class="code-block">
                    <pre><code class="language-bash">
git clone https://github.com/your-repo/llm-scanner
cd llm-scanner
pip install -r requirements.txt
                    </code></pre>
                </div>
                <p class="text-gray-600 mb-4">Once you’ve set up the environment, run the scanner:</p>
                <div class="code-block">
                    <pre><code class="language-bash">
python adaptive_scanner.py --model gpt-3.5 --provider OpenAI --endpoint https://api.openai.com/v1/chat/completions --categories "Prompt Injection" "Jailbreak"
                    </code></pre>
                </div>
                <p class="text-gray-600 mb-4">To keep the scanner running, consider using a service like <strong>systemd</strong> or running it in a detached screen session.</p>
            </div>

        </div>
    </div>
</section>

<!-- team -->
<section id="team" class="py-20 bg-white">
    <div class="container mx-auto px-4">
        <h2 class="text-3xl md:text-4xl font-bold text-center mb-12" data-aos="fade-up">Our Team</h2>
        <div class="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-3 gap-8">
            <div class="feature-card" data-aos="fade-up">
                <div class="p-6 flex flex-col">
                    <div class="flex items-center">
                        <h3 class="text-xl font-bold mb-2">Veera Swamy Gattupalli</h3>
                        <a href="https://www.linkedin.com/in/gattupalli-veeraswamy/" class="ml-2" aria-label="Veera Swamy Gattupalli LinkedIn">
                            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="#0A66C2">
                                <path d="M19 0h-14c-2.761 0-5 2.239-5 5v14c0 2.761 2.239 5 5 5h14c2.762 0 5-2.239 5-5v-14c0-2.761-2.238-5-5-5zm-11 19h-3v-11h3v11zm-1.5-12.268c-.966 0-1.75-.79-1.75-1.764s.784-1.764 1.75-1.764 1.75.79 1.75 1.764-.783 1.764-1.75 1.764zm13.5 12.268h-3v-5.604c0-1.337-.03-3.06-1.867-3.06-1.867 0-2.153 1.459-2.153 2.967v5.697h-3v-11h2.878v1.497h.041c.4-.757 1.378-1.557 2.834-1.557 3.03 0 3.587 1.996 3.587 4.592v6.468z"/>
                            </svg>
                        </a>
                    </div>
                    <a href="mailto:vgatt6@unh.newhaven.edu" class="text-sm text-gray-600 hover:underline">vgatt6@unh.newhaven.edu</a>
                </div>
            </div>
            <div class="feature-card" data-aos="fade-up" data-aos-delay="100">
                <div class="p-6 flex flex-col">
                    <div class="flex items-center">
                        <h3 class="text-xl font-bold mb-2">Likith Pawan Kagita</h3>
                        <a href="https://www.linkedin.com/in/likith-kagita-b11511220/" class="ml-2" aria-label="Likith Pawan Kagita LinkedIn">
                            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="#0A66C2">
                                <path d="M19 0h-14c-2.761 0-5 2.239-5 5v14c0 2.761 2.239 5 5 5h14c2.762 0 5-2.239 5-5v-14c0-2.761-2.238-5-5-5zm-11 19h-3v-11h3v11zm-1.5-12.268c-.966 0-1.75-.79-1.75-1.764s.784-1.764 1.75-1.764 1.75.79 1.75 1.764-.783 1.764-1.75 1.764zm13.5 12.268h-3v-5.604c0-1.337-.03-3.06-1.867-3.06-1.867 0-2.153 1.459-2.153 2.967v5.697h-3v-11h2.878v1.497h.041c.4-.757 1.378-1.557 2.834-1.557 3.03 0 3.587 1.996 3.587 4.592v6.468z"/>
                            </svg>
                        </a>
                    </div>
                    <a href="mailto:lkagi1@unh.newhaven.edu" class="text-sm text-gray-600 hover:underline">lkagi1@unh.newhaven.edu</a>
                </div>
            </div>
            <div class="feature-card" data-aos="fade-up" data-aos-delay="200">
                <div class="p-6 flex flex-col">
                    <div class="flex items-center">
                        <h3 class="text-xl font-bold mb-2">Gnana Vardhan Siddu Paruvada</h3>
                        <a href="https://www.linkedin.com/in/pgvsiddu" class="ml-2" aria-label="Gnana Vardhan Siddu Paruvada LinkedIn">
                            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="#0A66C2">
                                <path d="M19 0h-14c-2.761 0-5 2.239-5 5v14c0 2.761 2.239 5 5 5h14c2.762 0 5-2.239 5-5v-14c0-2.761-2.238-5-5-5zm-11 19h-3v-11h3v11zm-1.5-12.268c-.966 0-1.75-.79-1.75-1.764s.784-1.764 1.75-1.764 1.75.79 1.75 1.764-.783 1.764-1.75 1.764zm13.5 12.268h-3v-5.604c0-1.337-.03-3.06-1.867-3.06-1.867 0-2.153 1.459-2.153 2.967v5.697h-3v-11h2.878v1.497h.041c.4-.757 1.378-1.557 2.834-1.557 3.03 0 3.587 1.996 3.587 4.592v6.468z"/>
                            </svg>
                        </a>
                    </div>
                    <a href="mailto:gparu1@unh.newhaven.edu" class="text-sm text-gray-600 hover:underline">gparu1@unh.newhaven.edu</a>
                </div>
            </div>
            <div class="feature-card" data-aos="fade-up">
                <div class="p-6 flex flex-col">
                    <div class="flex items-center">
                        <h3 class="text-xl font-bold mb-2">Avinash Reddy Polireddy</h3>
                        <a href="https://www.linkedin.com/in/avinash-polireddy-4754b2194/" class="ml-2" aria-label="Avinash Reddy Polireddy LinkedIn">
                            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="#0A66C2">
                                <path d="M19 0h-14c-2.761 0-5 2.239-5 5v14c0 2.761 2.239 5 5 5h14c2.762 0 5-2.239 5-5v-14c0-2.761-2.238-5-5-5zm-11 19h-3v-11h3v11zm-1.5-12.268c-.966 0-1.75-.79-1.75-1.764s.784-1.764 1.75-1.764 1.75.79 1.75 1.764-.783 1.764-1.75 1.764zm13.5 12.268h-3v-5.604c0-1.337-.03-3.06-1.867-3.06-1.867 0-2.153 1.459-2.153 2.967v5.697h-3v-11h2.878v1.497h.041c.4-.757 1.378-1.557 2.834-1.557 3.03 0 3.587 1.996 3.587 4.592v6.468z"/>
                            </svg>
                        </a>
                    </div>
                    <a href="mailto:apoli6@unh.newhaven.edu" class="text-sm text-gray-600 hover:underline">apoli6@unh.newhaven.edu</a>
                </div>
            </div>
            <div class="feature-card" data-aos="fade-up" data-aos-delay="100">
                <div class="p-6 flex flex-col">
                    <div class="flex items-center">
                        <h3 class="text-xl font-bold mb-2">Sai Kumar Reddy Tummeti</h3>
                        <a href="https://www.linkedin.com/in/sai-kumar-reddy-tummeti-536701202" class="ml-2" aria-label="Sai Kumar Reddy Tummeti LinkedIn">
                            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="#0A66C2">
                                <path d="M19 0h-14c-2.761 0-5 2.239-5 5v14c0 2.761 2.239 5 5 5h14c2.762 0 5-2.239 5-5v-14c0-2.761-2.238-5-5-5zm-11 19h-3v-11h3v11zm-1.5-12.268c-.966 0-1.75-.79-1.75-1.764s.784-1.764 1.75-1.764 1.75.79 1.75 1.764-.783 1.764-1.75 1.764zm13.5 12.268h-3v-5.604c0-1.337-.03-3.06-1.867-3.06-1.867 0-2.153 1.459-2.153 2.967v5.697h-3v-11h2.878v1.497h.041c.4-.757 1.378-1.557 2.834-1.557 3.03 0 3.587 1.996 3.587 4.592v6.468z"/>
                            </svg>
                        </a>
                    </div>
                    <a href="mailto:stumm14@unh.newhaven.edu" class="text-sm text-gray-600 hover:underline">stumm14@unh.newhaven.edu</a>
                </div>
            </div>
            <div class="feature-card" data-aos="fade-up" data-aos-delay="200">
                <div class="p-6 flex flex-col">
                    <div class="flex items-center">
                        <h3 class="text-xl font-bold mb-2">Sahithi Kantu</h3>
                        <a href="https://www.linkedin.com/in/sahithi-kantu-36231b206/" class="ml-2" aria-label="Sahithi Kantu LinkedIn">
                            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="#0A66C2">
                                <path d="M19 0h-14c-2.761 0-5 2.239-5 5v14c0 2.761 2.239 5 5 5h14c2.762 0 5-2.239 5-5v-14c0-2.761-2.238-5-5-5zm-11 19h-3v-11h3v11zm-1.5-12.268c-.966 0-1.75-.79-1.75-1.764s.784-1.764 1.75-1.764 1.75.79 1.75 1.764-.783 1.764-1.75 1.764zm13.5 12.268h-3v-5.604c0-1.337-.03-3.06-1.867-3.06-1.867 0-2.153 1.459-2.153 2.967v5.697h-3v-11h2.878v1.497h.041c.4-.757 1.378-1.557 2.834-1.557 3.03 0 3.587 1.996 3.587 4.592v6.468z"/>
                            </svg>
                        </a>
                    </div>
                    <a href="mailto:skant8@unh.newhaven.edu" class="text-sm text-gray-600 hover:underline">skant8@unh.newhaven.edu</a>
                </div>
            </div>
            <div class="feature-card" data-aos="fade-up">
                <div class="p-6 flex flex-col">
                    <div class="flex items-center">
                        <h3 class="text-xl font-bold mb-2">Bhaskara Vijaya Sai Swamy Vanacharla</h3>
                        <a href="https://www.linkedin.com/in/vanacharla-bhaskara-226740169/" class="ml-2" aria-label="Bhaskara Vijaya Sai Swamy Vanacharla LinkedIn">
                            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="#0A66C2">
                                <path d="M19 0h-14c-2.761 0-5 2.239-5 5v14c0 2.761 2.239 5 5 5h14c2.762 0 5-2.239 5-5v-14c0-2.761-2.238-5-5-5zm-11 19h-3v-11h3v11zm-1.5-12.268c-.966 0-1.75-.79-1.75-1.764s.784-1.764 1.75-1.764 1.75.79 1.75 1.764-.783 1.764-1.75 1.764zm13.5 12.268h-3v-5.604c0-1.337-.03-3.06-1.867-3.06-1.867 0-2.153 1.459-2.153 2.967v5.697h-3v-11h2.878v1.497h.041c.4-.757 1.378-1.557 2.834-1.557 3.03 0 3.587 1.996 3.587 4.592v6.468z"/>
                            </svg>
                        </a>
                    </div>
                    <a href="mailto:bvana2@unh.newhaven.edu" class="text-sm text-gray-600 hover:underline">bvana2@unh.newhaven.edu</a>
                </div>
            </div>
            <div class="feature-card" data-aos="fade-up" data-aos10px;">
                <div class="p-6 flex flex-col">
                    <div class="flex items-center">
                        <h3 class="text-xl font-bold mb-2">Kiran Varma Ceemala Shankar</h3>
                        <a href="https://www.linkedin.com/in/cskiranvarma" class="ml-2" aria-label="Kiran Varma Ceemala Shankar LinkedIn">
                            <svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="#0A66C2">
                                <path d="M19 0h-14c-2.761 0-5 2.239-5 5v14c0 2.761 2.239 5 5 5h14c2.762 0 5-2.239 5-5v-14c0-2.761-2.238-5-5-5zm-11 19h-3v-11h3v11zm-1.5-12.268c-.966 0-1.75-.79-1.75-1.764s.784-1.764 1.75-1.764 1.75.79 1.75 1.764-.783 1.764-1.75 1.764zm13.5 12.268h-3v-5.604c0-1.337-.03-3.06-1.867-3.06-1.867 0-2.153 1.459-2.153 2.967v5.697h-3v-11h2.878v1.497h.041c.4-.757 1.378-1.557 2.834-1.557 3.03 0 3.587 1.996 3.587 4.592v6.468z"/>
                            </svg>
                        </a>
                    </div>
                    <a href="mailto:kceem1@unh.newhaven.edu" class="text-sm text-gray-600 hover:underline">kceem1@unh.newhaven.edu</a>
                </div>
            </div>
        </div>
    </div>
</section>

              <!-- FAQ Section -->
<section id="faq" class="py-20 bg-gray-50">
    <div class="container mx-auto px-4">
        <h2 class="text-3xl md:text-4xl font-bold text-center mb-12" data-aos="fade-up">Frequently Asked Questions</h2>
        <div class="max-w-3xl mx-auto space-y-4 faq-container">
            <div class="faq-item">
                <button class="faq-question" aria-expanded="false" aria-controls="faq-answer-1">
                    <span>What is the purpose of the LLM Vulnerability Scanner?</span>
                    <i class="fas fa-chevron-down transition-transform"></i>
                </button>
                <div id="faq-answer-1" class="faq-answer">
                    <p>The LLM Vulnerability Scanner tests large language models (LLMs) for vulnerabilities like prompt injection, jailbreaking, privacy leaks, and more. It uses adaptive prompt generation and Llama Guard to evaluate prompt and response safety, helping developers identify and mitigate security risks in LLMs.</p>
                </div>
            </div>
            <div class="faq-item">
                <button class="faq-question" aria-expanded="false" aria-controls="faq-answer-2">
                    <span>Who is the intended audience for this project?</span>
                    <i class="fas fa-chevron-down transition-transform"></i>
                </button>
                <div id="faq-answer-2" class="faq-answer">
                    <p>AI developers, security researchers, and organizations deploying LLMs who need to assess model robustness against malicious inputs or unintended behaviors.</p>
                </div>
            </div>
            <div class="faq-item">
                <button class="faq-question" aria-expanded="false" aria-controls="faq-answer-3">
                    <span>What types of vulnerabilities does the scanner test for?</span>
                    <i class="fas fa-chevron-down transition-transform"></i>
                </button>
                <div id="faq-answer-3" class="faq-answer">
                    <p>It tests for Prompt Injection, Jailbreak, Privacy Leak, Denial of Service, Command Execution, and System Takeover, as defined in <a href="#documentation" class="text-blue-600 hover:underline">template.json</a> and <a href="#documentation" class="text-blue-600 hover:underline">llm_vulnerability_prompt_templates_variable.json</a>.</p>
                </div>
            </div>
            <div class="faq-item">
                <button class="faq-question" aria-expanded="false" aria-controls="faq-answer-4">
                    <span>What technologies are used in the project?</span>
                    <i class="fas fa-chevron-down transition-transform"></i>
                </button>
                <div id="faq-answer-4" class="faq-answer">
                    <p>The project uses Python with libraries like <code>openai</code>, <code>requests</code>, <code>streamlit</code>, <code>pandas</code>, <code>plotly</code>, and <code>ollama</code> (see <a href="#documentation" class="text-blue-600 hover:underline">requirements.txt</a>). It integrates Llama Guard for safety checks and OpenAI for prompt generation and evaluation. The UI uses Tailwind CSS and custom styles (<a href="#documentation" class="text-blue-600 hover:underline">style.css</a>).</p>
                </div>
            </div>
            <div class="faq-item">
                <button class="faq-question" aria-expanded="false" aria-controls="faq-answer-5">
                    <span>How does Llama Guard contribute to the scanner?</span>
                    <i class="fas fa-chevron-down transition-transform"></i>
                </button>
                <div id="faq-answer-5" class="faq-answer">
                    <p>Llama Guard, hosted locally via Ollama, evaluates prompts and responses for safety, detecting unsafe categories like injection or privacy violations. It provides detailed safety reports, as implemented in <a href="#documentation" class="text-blue-600 hover:underline">llama_guard.py</a>.</p>
                </div>
            </div>
            <div class="faq-item">
                <button class="faq-question" aria-expanded="false" aria-controls="faq-answer-6">
                    <span>What is the role of the adaptive scanning mechanism?</span>
                    <i class="fas fa-chevron-down transition-transform"></i>
                </button>
                <div id="faq-answer-6" class="faq-answer">
                    <p>The scanner (<a href="#documentation" class="text-blue-600 hover:underline">adaptive_scanner.py</a>) generates evolving prompts using OpenAI’s GPT model to test LLMs iteratively. It adapts prompts based on previous responses to uncover vulnerabilities, stopping when a high-risk score (default threshold: 6/10) is detected or after 6 attempts.</p>
                </div>
            </div>
            <div class="faq-item">
                <button class="faq-question" aria-expanded="false" aria-controls="faq-answer-7">
                    <span>How are risk scores calculated?</span>
                    <i class="fas fa-chevron-down transition-transform"></i>
                </button>
                <div id="faq-answer-7" class="faq-answer">
                    <p>OpenAI’s GPT-3.5-turbo evaluates responses and assigns a risk score (0–10) based on potential security risks. The score is extracted using LLM-based parsing in <a href="#documentation" class="text-blue-600 hover:underline">adaptive_scanner.py</a>.</p>
                </div>
            </div>
            <div class="faq-item">
                <button class="faq-question" aria-expanded="false" aria-controls="faq-answer-8">
                    <span>What is the significance of the CSV file?</span>
                    <i class="fas fa-chevron-down transition-transform"></i>
                </button>
                <div id="faq-answer-8" class="faq-answer">
                    <p>The CSV (<a href="#documentation" class="text-blue-600 hover:underline">llm_vulnerability_report_prompt_injection.csv</a>) logs scan results, including timestamps, attack types, prompts, responses, evaluations, risk scores, and Llama Guard reports. It provides a historical record of vulnerability tests for analysis.</p>
                </div>
            </div>
            <div class="faq-item">
                <button class="faq-question" aria-expanded="false" aria-controls="faq-answer-9">
                    <span>Can the scanner test LLMs from different providers?</span>
                    <i class="fas fa-chevron-down transition-transform"></i>
                </button>
                <div id="faq-answer-9" class="faq-answer">
                    <p>Yes, it supports providers like OpenAI, Gemini, Claude, Mistral, Ollama, and Together, configurable via command-line arguments in <a href="#documentation" class="text-blue-600 hover:underline">adaptive_scanner.py</a>.</p>
                </div>
            </div>
            <div class="faq-item">
                <button class="faq-question" aria-expanded="false" aria-controls="faq-answer-11">
                    <span>How do I set up the project locally?</span>
                    <i class="fas fa-chevron-down transition-transform"></i>
                </button>
                <div id="faq-answer-11" class="faq-answer">
                    <p>Clone the repository, install dependencies from <a href="#documentation" class="text-blue-600 hover:underline">requirements.txt</a> (<code>pip install -r requirements.txt</code>), set up environment variables (e.g., <code>OPENAI_API_KEY</code> in a <code>.env</code> file), and run <a href="#documentation" class="text-blue-600 hover:underline">adaptive_scanner.py</a> with appropriate arguments (e.g., <code>--model</code>, <code>--provider</code>).</p>
                </div>
            </div>
            <div class="faq-item">
                <button class="faq-question" aria-expanded="false" aria-controls="faq-answer-12">
                    <span>What are the command-line arguments for running the scanner?</span>
                    <i class="fas fa-chevron-down transition-transform"></i>
                </button>
                <div id="faq-answer-12" class="faq-answer">
                    <p>Key arguments include <code>--model</code> (target model), <code>--provider</code> (e.g., Ollama), <code>--endpoint</code> (API URL), <code>--llama-guard</code> (enable Llama Guard), <code>--categories</code> (vulnerability types), and <code>--output</code> (results directory). See <a href="#documentation" class="text-blue-600 hover:underline">adaptive_scanner.py</a> for details.</p>
                </div>
            </div>
            <div class="faq-item">
                <button class="faq-question" aria-expanded="false" aria-controls="faq-answer-13">
                    <span>How can I view the scan results?</span>
                    <i class="fas fa-chevron-down transition-transform"></i>
                </button>
                <div id="faq-answer-13" class="faq-answer">
                    <p>Results are saved as JSON files in the specified output directory (e.g., <code>results/vulnerability_scan_&lt;model&gt;_&lt;timestamp&gt;.json</code>). Detailed Llama Guard reports are saved in the <code>llama_guard_reports</code> directory if enabled.</p>
                </div>
            </div>
            <div class="faq-item">
                <button class="faq-question" aria-expanded="false" aria-controls="faq-answer-14">
                    <span>Can I customize the vulnerability templates?</span>
                    <i class="fas fa-chevron-down transition-transform"></i>
                </button>
                <div id="faq-answer-14" class="faq-answer">
                    <p>Yes, edit <a href="#documentation" class="text-blue-600 hover:underline">llm_vulnerability_prompt_templates_variable.json</a> or <a href="#documentation" class="text-blue-600 hover:underline">template.json</a> to add or modify attack templates. Each template specifies an attack type and prompt structure.</p>
                </div>
            </div>
            <div class="faq-item">
                <button class="faq-question" aria-expanded="false" aria-controls="faq-answer-15">
                    <span>What happens if Llama Guard is unavailable?</span>
                    <i class="fas fa-chevron-down transition-transform"></i>
                </button>
                <div id="faq-answer-15" class="faq-answer">
                    <p>If Llama Guard fails to connect (tested via <code>test_llama_guard_connection</code> in <a href="#documentation" class="text-blue-600 hover:underline">llama_guard.py</a>), the scanner continues without it, relying solely on OpenAI evaluations.</p>
                </div>
            </div>
            <div class="faq-item">
                <button class="faq-question" aria-expanded="false" aria-controls="faq-answer-16">
                    <span>Is it safe to use this scanner on production LLMs?</span>
                    <i class="fas fa-chevron-down transition-transform"></i>
                </button>
                <div id="faq-answer-16" class="faq-answer">
                    <p>The scanner is designed for testing in controlled environments. Use caution in production, as it generates potentially malicious prompts that could trigger unintended behaviors.</p>
                </div>
            </div>
            <div class="faq-item">
                <button class="faq-question" aria-expanded="false" aria-controls="faq-answer-17">
                    <span>How does the scanner prevent accidental harm?</span>
                    <i class="fas fa-chevron-down transition-transform"></i>
                </button>
                <div id="faq-answer-17" class="faq-answer">
                    <p>Llama Guard flags unsafe prompts and responses, and the scanner operates within a defined scope of attack categories. However, users should test in sandboxed environments to avoid real-world impact.</p>
                </div>
            </div>
            <div class="faq-item">
                <button class="faq-question" aria-expanded="false" aria-controls="faq-answer-18">
                    <span>What does a high risk score indicate?</span>
                    <i class="fas fa-chevron-down transition-transform"></i>
                </button>
                <div id="faq-answer-18" class="faq-answer">
                    <p>A score of 6 or higher (out of 10) suggests a significant vulnerability, such as leaking sensitive data or executing harmful instructions, as seen in the CSV data (e.g., risk score 9 for privacy leaks).</p>
                </div>
            </div>
            </div>
        </div>
    </div>
</section>


        <!-- Footer -->
        <footer class="bg-blue-900 text-white py-12">
            <div class="container mx-auto px-4">
                <div class="grid grid-cols-1 md:grid-cols-4 gap-8">
                    <div>
                        <h3 class="text-xl font-bold mb-4">LLM Scanner</h3>
                        <p class="text-gray-300">Securing the future of AI, one model at a time.</p>
                    </div>
                    <div>
                        <h4 class="font-bold mb-4">Quick Links</h4>
                        <ul class="space-y-2">
                            <li><a href="#features" class="text-gray-300 hover:text-white">Features</a></li>
                            <li><a href="#documentation" class="text-gray-300 hover:text-white">Documentation</a></li>
                            <li><a href="#deployment" class="text-gray-300 hover:text-white">Deployment</a></li>
                        </ul>
                    </div>
                    <div>
                        <h4 class="font-bold mb-4">Resources</h4>
                        <ul class="space-y-2">
                            <li><a href="https://www.notion.so/Capstone-19c8741769b6804891c4f828066b3f46?pvs=4" class="text-gray-300 hover:text-white" target="_blank" rel="noopener noreferrer">Blog</a></li>
                            <li><a href="#" class="text-gray-300 hover:text-white">Tutorials</a></li>
                            <li><a href="#api-reference" class="text-gray-300 hover:text-white">API Reference</a></li>
                        </ul>
                    </div>
                    <div>
                        <h4 class="font-bold mb-4">Contact</h4>
                        <ul class="space-y-2">
                            <li><a href="mailto:vgatt6@unh.newhaven.edu" class="text-gray-300 hover:text-white">vgatt6@unh.newhaven.edu</a></li>
                            <li><a href="https://github.com/UNH-TCOE-ECECS/S25-S1-Team5" class="text-gray-300 hover:text-white">GitHub</a></li>
                        </ul>
                    </div>
                </div>
                <div class="mt-8 pt-8 border-t border-blue-800 text-center">
                    <p class="text-gray-300">&copy; 2025 LLM Vulnerability Scanner. All rights reserved.</p>
                </div>
            </div>
        </footer>
    </div>

    <script defer src="https://unpkg.com/aos@2.3.1/dist/aos.js"></script>
    <script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
    <script src="main.js"></script>
</body>
</html>
